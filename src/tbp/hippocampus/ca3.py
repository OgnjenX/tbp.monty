"""CA3 region implementation with autoassociative memory and pattern completion.

CA3 is a recurrent attractor network that:
1. Receives sparse inputs from Dentate Gyrus (mossy fibers)
2. Forms autoassociative memories via recurrent collaterals
3. Performs pattern completion from partial cues
4. Supports memory replay/reactivation during consolidation
5. Maintains transition graph for Successor Representation (SR)

In this refactored version, replay and planning emerge directly from
DG → CA3 → CA1 dynamics instead of explicit graph search:

- DG produces sparse distributed representations (SDRs) for pattern separation
- CA3 stores both autoassociative and *transition* structure in a recurrent
  weight matrix W, updated with local Hebbian rules
- Forward / backward replay and goal-directed trajectories are generated by
  iterating CA3 dynamics (step()) over SDRs, rather than BFS over a Python
  transition graph

Key biological features modeled:
- Recurrent collateral connections (~2% connectivity in rodents)
- Attractor dynamics for pattern completion and replay
- Sharp-wave ripple–like sequence generation during replay
- NMDA-dependent synaptic plasticity
- Local transition learning between sequential states (SR foundation)

TEM/SR extensions:
- HState: abstract latent states that can represent spatial OR abstract context
- Transition graph utilities are retained for diagnostics, but the *core*
  replay and planning API now rely on recurrent dynamics over SDRs.
"""

from dataclasses import dataclass
from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union

import numpy as np

from .types import SpatialEvent

if TYPE_CHECKING:
    from .hstate import HState


@dataclass
class CA3Config:
    """Configuration for CA3 autoassociative network.

    Attributes:
        n_pyramidal_cells: Number of CA3 pyramidal neurons (default ~250k in rodents).
        n_active_cells: Target number of active cells per pattern (~1-2% sparsity).
        recurrent_connectivity: Fraction of recurrent connections (~2% in rodents).
        learning_rate: Hebbian learning rate for recurrent weights.
        pattern_completion_threshold: Activation threshold for pattern completion.
        max_iterations: Maximum attractor iterations for convergence.
        convergence_threshold: Threshold for attractor convergence.
        noise_level: Noise added during retrieval for generalization.
        memory_capacity: Approximate number of patterns before interference.
        transition_learning_rate: Learning rate for transition graph updates.
        transition_decay: Decay factor for old transitions (0=no decay, 1=full decay).
        sr_gamma: Discount factor for multi-step SR predictions (0-1).
        value_learning_rate: Learning rate for value/reward TD updates.
        goal_modulation_gain: Gain for per-unit goal modulation during replay.
        value_modulation_gain: Gain for per-unit value modulation during replay.
    """

    n_pyramidal_cells: int = 2500  # Scaled down from biological ~250k
    n_active_cells: int = 50  # ~2% sparsity
    recurrent_connectivity: float = 0.02
    learning_rate: float = 0.1
    pattern_completion_threshold: float = 0.3
    max_iterations: int = 10
    convergence_threshold: float = 0.01
    noise_level: float = 0.01
    memory_capacity: int = 500
    # SR/Transition graph parameters
    transition_learning_rate: float = 0.1
    transition_decay: float = 0.01
    sr_gamma: float = 0.9
    value_learning_rate: float = 0.1
    # Local modulation gains for dynamics-based replay/planning
    goal_modulation_gain: float = 1.0
    value_modulation_gain: float = 1.0


@dataclass
class CA3Memory:
    """A stored memory in CA3.

    Attributes:
        pattern: The stored SDR pattern (binary).
        event: Associated spatial event.
        timestamp: When the memory was stored.
        retrieval_count: How many times this memory was retrieved.
        strength: Memory strength (consolidation level).
        hstate_id: Optional ID of associated HState.
        basis_vector: Optional EC basis vector for this memory.
    """

    pattern: np.ndarray
    event: SpatialEvent
    timestamp: float
    retrieval_count: int = 0
    strength: float = 1.0
    hstate_id: Optional[str] = None
    basis_vector: Optional[np.ndarray] = None


class CA3:
    """CA3 autoassociative network with recurrent collaterals.

    CA3 implements a recurrent attractor network that can:
    - Store DG-encoded patterns in autoassociative weights
    - Complete partial patterns from cues via attractor dynamics
    - Replay stored sequences during consolidation (forward/backward)
    - Support creative recombination via mixed initial conditions
    - Generate HState latent representations from SDR activity

    All temporal structure is encoded in the recurrent weight matrix W.
    There is no separate symbolic transition graph or global path search:
    replay and planning emerge directly from iterating CA3.step() over
    sparse distributed representations (SDRs), optionally modulated by
    local goal/value biases.

    Example:
        >>> config = CA3Config(n_pyramidal_cells=1000)
        >>> ca3 = CA3(config)
        >>> # Store a pattern from DG
        >>> ca3.store(dg_pattern, spatial_event)
        >>> # Later, complete from partial cue
        >>> completed, retrieved_event = ca3.pattern_complete(partial_cue)
        >>> # Predict future states
        >>> future = ca3.predict_future(current_hstate, n_steps=5)
    """

    def __init__(self, config: Optional[CA3Config] = None, seed: Optional[int] = None):
        """Initialize CA3 network.

        Args:
            config: CA3 configuration. Uses defaults if not provided.
            seed: Random seed for reproducibility. If None, uses system entropy.
        """
        self.config = config or CA3Config()
        self._rng = np.random.default_rng(seed)

        # Recurrent weight matrix (sparse topology, dense values).
        # This encodes BOTH autoassociative attractors (pattern completion)
        # and directed transitions between successive SDRs observed over time.
        n = self.config.n_pyramidal_cells
        n_connections = int(n * n * self.config.recurrent_connectivity)

        # Initialize recurrent weights to zero; connectivity mask fixes topology.
        self._recurrent_weights = np.zeros((n, n), dtype=np.float32)

        # Create random sparse connectivity pattern (fixed topology)
        self._connection_mask = self._create_sparse_mask(n, n_connections)

        # Memory storage (CA3 patterns and associated events)
        self._memories: List[CA3Memory] = []
        self._pattern_to_memory: Dict[Tuple, int] = {}  # Hash -> memory index

        # Statistics
        self._total_stores = 0
        self._total_retrievals = 0
        self._successful_completions = 0

    def _create_sparse_mask(
            self, n: int, n_connections: int
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Create sparse connectivity mask.

        Args:
            n: Number of neurons.
            n_connections: Target number of connections.

        Returns:
            Tuple of (row_indices, col_indices) for sparse connections.
        """
        # Random connections excluding self-connections
        rows = self._rng.integers(0, n, size=n_connections)
        cols = self._rng.integers(0, n, size=n_connections)

        # Remove self-connections
        valid = rows != cols
        return rows[valid], cols[valid]

    # ==================== Core Dynamics Helpers ====================

    def _k_wta(self, activations: np.ndarray, k: int) -> np.ndarray:
        """k-Winner-Take-All sparsification (SDR generation).

        This implements local recurrent inhibition in CA3: only the k most
        strongly driven pyramidal cells remain active, yielding a sparse
        distributed representation that can serve as an attractor state.
        """
        activations = np.asarray(activations, dtype=np.float32).ravel()
        n = activations.shape[0]
        if k <= 0:
            return np.zeros_like(activations)
        if k >= n:
            # Degenerate case: all cells active
            return (activations > 0).astype(np.float32)

        # Select indices of top-k activations
        top_indices = np.argpartition(activations, -k)[-k:]
        sdr = np.zeros_like(activations)
        sdr[top_indices] = 1.0
        return sdr

    def step(
            self,
            x_current: np.ndarray,
            goal_mod: Optional[np.ndarray] = None,
            value_mod: Optional[np.ndarray] = None,
            reverse: bool = False,
    ) -> np.ndarray:
        """One CA3 dynamic step: x_{t+1} = kWTA(W x_t).

        This is the core mechanistic update used for replay and planning.
        It performs:

        1. Recurrent drive from current pattern via W or Wᵀ
        2. Optional local modulation by goal/value bias vectors
        3. k-WTA sparsification to produce next SDR pattern

        Args:
            x_current: Current CA3 activity pattern (SDR, shape (n_cells,)).
            goal_mod: Optional per-unit goal modulation vector. Higher values
                increase excitability of units consistent with the goal.
            value_mod: Optional per-unit value modulation vector. Higher values
                bias replay toward high-value states.
            reverse: If True, use Wᵀ for backward replay (predecessor search).

        Returns:
            Next CA3 SDR pattern after one dynamic step.
        """
        x = np.asarray(x_current, dtype=np.float32).ravel()
        if x.shape[0] != self.config.n_pyramidal_cells:
            raise ValueError(
                f"x_current must have shape ({self.config.n_pyramidal_cells},), "
                f"got {x.shape}"
            )

        # Recurrent input: forward (successor) vs backward (predecessor) replay
        if reverse:
            recurrent_input = self._recurrent_weights.T @ x
        else:
            recurrent_input = self._recurrent_weights @ x

        # Local multiplicative modulation by goal and value (no global path scoring).
        # In more structured settings, goal_mod / value_mod can be produced via
        # learned projection matrices W_goal, W_value from low-dimensional goal
        # or value vectors into CA3 unit space.
        if goal_mod is not None:
            gm = np.asarray(goal_mod, dtype=np.float32).ravel()
            if gm.shape != recurrent_input.shape:
                raise ValueError(
                    f"goal_mod must have shape {recurrent_input.shape}, got {gm.shape}"
                )
            recurrent_input *= (1.0 + self.config.goal_modulation_gain * gm)

        if value_mod is not None:
            vm = np.asarray(value_mod, dtype=np.float32).ravel()
            if vm.shape != recurrent_input.shape:
                raise ValueError(
                    f"value_mod must have shape {recurrent_input.shape}, got {vm.shape}"
                )
            recurrent_input *= (1.0 + self.config.value_modulation_gain * vm)

        # k-Winner-Take-All to produce next sparse CA3 code
        x_next = self._k_wta(recurrent_input, self.config.n_active_cells)
        return x_next

    def store(
            self,
            dg_pattern: np.ndarray,
            event: SpatialEvent,
            ca3_pattern: Optional[np.ndarray] = None,
    ) -> bool:
        """Store a pattern from Dentate Gyrus.

        Uses Hebbian learning to strengthen connections between
        co-active neurons in the input pattern.

        Args:
            dg_pattern: Sparse binary pattern from DG.
            event: Associated spatial event.
            ca3_pattern: Optional precomputed CA3 pattern. If provided, bypasses
                projection to avoid redundant computation.

        Returns:
            True if stored as new pattern, False if existing pattern was strengthened.
        """
        # Project DG pattern to CA3 space if needed
        if ca3_pattern is None:
            ca3_pattern = self._project_from_dg(dg_pattern)
        else:
            ca3_pattern = np.asarray(ca3_pattern, dtype=np.float32)

        # Check if pattern already exists (update strength)
        pattern_hash = self._pattern_hash(ca3_pattern)
        if pattern_hash in self._pattern_to_memory:
            idx = self._pattern_to_memory[pattern_hash]
            self._memories[idx].strength += 0.1
            return False

        # Hebbian learning on recurrent weights (autoassociative attractor)
        # Strengthen connections between co-active neurons within the SAME pattern.
        # This models CA3 recurrent collaterals forming point-attractor basins.
        active_indices = np.nonzero(ca3_pattern > 0)[0]
        if active_indices.size > 0:
            # Outer product restricted to existing connectivity mask
            rows, cols = self._connection_mask
            pre = ca3_pattern[cols]
            post = ca3_pattern[rows]
            delta = self.config.learning_rate * (post * pre)
            self._recurrent_weights[rows, cols] += delta

            # Normalize weights to prevent unbounded growth
            max_weight = float(np.max(np.abs(self._recurrent_weights)))
            if max_weight > 1.0:
                self._recurrent_weights /= max_weight

        # Store memory (CA3 attractor pattern)
        memory = CA3Memory(
            pattern=ca3_pattern.copy(),
            event=event,
            timestamp=event.timestamp,
        )
        self._memories.append(memory)
        self._pattern_to_memory[pattern_hash] = len(self._memories) - 1

        self._total_stores += 1

        # Check capacity and consolidate if needed
        if len(self._memories) > self.config.memory_capacity:
            self._consolidate()

        return True

    def _project_from_dg(self, dg_pattern: np.ndarray) -> np.ndarray:
        """Project DG pattern to CA3 representation.

        Mossy fiber projection from DG to CA3 is sparse but strong.

        Args:
            dg_pattern: Pattern from Dentate Gyrus.

        Returns:
            CA3 pattern (same or projected dimensionality).
        """
        if len(dg_pattern) == self.config.n_pyramidal_cells:
            return dg_pattern

        # Project to CA3 dimensionality
        ca3_pattern = np.zeros(self.config.n_pyramidal_cells, dtype=np.float32)

        # Sparse random projection
        active_dg = np.nonzero(dg_pattern > 0)[0]
        n_ca3_active = min(
            len(active_dg) * 2, self.config.n_active_cells
        )  # Slight expansion

        if len(active_dg) > 0:
            # Each DG active neuron activates 2-3 CA3 neurons
            for _ in range(len(active_dg)):
                ca3_targets = self._rng.integers(
                    0, self.config.n_pyramidal_cells, size=2
                )
                ca3_pattern[ca3_targets] = 1.0

            # Ensure target sparsity
            if np.sum(ca3_pattern) > n_ca3_active:
                active = np.nonzero(ca3_pattern > 0)[0]
                keep = self._rng.choice(active, size=n_ca3_active, replace=False)
                ca3_pattern = np.zeros_like(ca3_pattern)
                ca3_pattern[keep] = 1.0

        return ca3_pattern

    def pattern_complete(
            self, partial_cue: np.ndarray, return_event: bool = True
    ) -> Tuple[np.ndarray, Optional[SpatialEvent]]:
        """Complete a pattern from partial cue using attractor dynamics.

        Iteratively updates the pattern based on recurrent weights
        until convergence or max iterations.

        Args:
            partial_cue: Partial or noisy pattern to complete.
            return_event: Whether to also return the associated event.

        Returns:
            Tuple of (completed_pattern, associated_event or None).
        """
        self._total_retrievals += 1

        # Project to CA3 if needed
        current = self._project_from_dg(partial_cue).astype(np.float32)

        # Add noise for generalization
        noise = self._rng.normal(0, self.config.noise_level, current.shape)
        current = current + noise

        # Attractor dynamics
        for iteration in range(self.config.max_iterations):
            # Compute next state from recurrent input
            recurrent_input = np.dot(self._recurrent_weights, current)

            # Apply sparsity constraint: always select top n_active_cells
            n_active = self.config.n_active_cells
            if n_active >= len(recurrent_input):
                # If more active cells than available, activate all
                new_pattern = np.ones_like(recurrent_input, dtype=np.float32)
            else:
                # Find indices of top n_active values
                top_indices = np.argpartition(recurrent_input, -n_active)[-n_active:]
                new_pattern = np.zeros_like(recurrent_input, dtype=np.float32)
                new_pattern[top_indices] = 1.0
            # Blend with input (partial cue influence)
            blend_factor = 1.0 / (iteration + 2)
            new_pattern = (1 - blend_factor) * new_pattern + blend_factor * (
                partial_cue[:self.config.n_pyramidal_cells] if len(partial_cue)
                                                               >= self.config.n_pyramidal_cells else current
            )

            # Check convergence
            change = np.mean(np.abs(new_pattern - current))
            current = new_pattern

            if change < self.config.convergence_threshold:
                break

        # Find best matching stored memory
        completed = (current > 0.5).astype(np.float32)
        event = None

        if return_event and self._memories:
            best_match_idx = self._find_best_match(completed)
            if best_match_idx >= 0:
                memory = self._memories[best_match_idx]
                memory.retrieval_count += 1
                event = memory.event
                self._successful_completions += 1

        return completed, event

    def _find_best_match(self, pattern: np.ndarray) -> int:
        """Find the memory that best matches a pattern.

        Args:
            pattern: Pattern to match.

        Returns:
            Index of best matching memory, or -1 if no good match.
        """
        if not self._memories:
            return -1

        best_overlap = 0.0
        best_idx = -1

        for idx, memory in enumerate(self._memories):
            overlap = np.sum(pattern * memory.pattern) / (
                    np.sum(pattern) + np.sum(memory.pattern) + 1e-10
            )
            if overlap > best_overlap and overlap > self.config.pattern_completion_threshold:
                best_overlap = overlap
                best_idx = idx

        return best_idx

    def replay(self, n_patterns: int = 5) -> List[Tuple[np.ndarray, SpatialEvent]]:
        """Replay stored memories (sharp-wave ripple simulation).

        During consolidation, CA3 spontaneously reactivates stored patterns.
        This is thought to help transfer memories to neocortex.

        Args:
            n_patterns: Number of patterns to replay.

        Returns:
            List of (pattern, event) tuples in replay order.
        """
        if not self._memories:
            return []

        # Weight by recency and strength for replay probability
        weights = np.array([
            m.strength * np.exp(-0.001 * (len(self._memories) - i))
            for i, m in enumerate(self._memories)
        ])
        weights = weights / weights.sum()

        # Sample memories for replay
        n_to_replay = min(n_patterns, len(self._memories))
        indices = self._rng.choice(
            len(self._memories),
            size=n_to_replay,
            replace=False,
            p=weights
        )

        replayed = []
        for idx in indices:
            memory = self._memories[idx]
            # Pattern completion during replay
            completed, event = self.pattern_complete(memory.pattern, return_event=True)
            if event is not None:
                replayed.append((completed, event))

        return replayed

    # (Extended replay and policy-like replay are now implemented at the
    #  Hippocampus level using CA3.step() and CA1 decoding.)

    def compute_novelty(self, pattern: np.ndarray) -> float:
        """Compute novelty score for a pattern.

        High novelty = poor pattern completion = new experience.
        Low novelty = good pattern completion = familiar experience.

        Args:
            pattern: Pattern to evaluate.

        Returns:
            Novelty score between 0 (familiar) and 1 (novel).
        """
        if not self._memories:
            return 1.0

        # Try pattern completion
        completed, _ = self.pattern_complete(pattern, return_event=False)

        # Novelty is inverse of best match quality
        best_overlap = 0.0
        for memory in self._memories:
            overlap = np.sum(completed * memory.pattern) / (
                    np.sum(completed) + np.sum(memory.pattern) + 1e-10
            )
            best_overlap = max(best_overlap, overlap)

        return 1.0 - best_overlap

    def _consolidate(self) -> None:
        """Consolidate memories, removing weak ones.

        Simulates synaptic consolidation where weak memories
        are pruned and strong ones are preserved.
        """
        # Remove memories with low strength and few retrievals
        threshold_strength = 0.3
        self._memories = [
            m for m in self._memories
            if m.strength > threshold_strength or m.retrieval_count > 2
        ]

        # Update index mapping
        self._pattern_to_memory = {
            self._pattern_hash(m.pattern): i
            for i, m in enumerate(self._memories)
        }

    def _pattern_hash(self, pattern: np.ndarray) -> tuple:
        """Create hashable representation of pattern.

        Args:
            pattern: Binary pattern.

        Returns:
            Tuple of active indices for hashing.
        """
        return tuple(np.nonzero(pattern > 0.5)[0])

    def learn_transition(self, prev_pattern: np.ndarray,
                         current_pattern: np.ndarray) -> None:
        """Hebbian transition learning between two CA3 patterns.

        Updates recurrent weights using a local rule:
            W += eta * outer(x_current, x_prev)
        restricted to the fixed connectivity mask.
        """
        prev = np.asarray(prev_pattern, dtype=np.float32).ravel()
        curr = np.asarray(current_pattern, dtype=np.float32).ravel()
        if prev.shape[0] != self.config.n_pyramidal_cells or curr.shape[0] != self.config.n_pyramidal_cells:
            raise ValueError("prev_pattern and current_pattern must match CA3 size")

        rows, cols = self._connection_mask
        pre = prev[cols]
        post = curr[rows]
        delta = self.config.learning_rate * (post * pre)
        self._recurrent_weights[rows, cols] += delta

        # Normalize to prevent runaway growth
        max_weight = float(np.max(np.abs(self._recurrent_weights)))
        if max_weight > 1.0:
            self._recurrent_weights /= max_weight

    @property
    def n_memories(self) -> int:
        """Number of stored memories."""
        return len(self._memories)

    @property
    def n_hstates(self) -> int:
        """Number of stored HStates (deprecated; always 0 in CA3).

        HStates are now managed at the Hippocampus/CA1 level.
        """
        return 0

    @property
    def hstates(self) -> List["HState"]:
        """All stored HStates (deprecated; empty in CA3)."""
        return []

    @property
    def current_hstate(self) -> Optional["HState"]:
        """Current HState (now managed by Hippocampus; always None here)."""
        return None

    @property
    def statistics(self) -> dict:
        """Get CA3 statistics.

        Returns:
            Dictionary with store/retrieval statistics.
        """
        return {
            "n_memories": len(self._memories),
            "n_hstates": 0,
            "total_stores": self._total_stores,
            "total_retrievals": self._total_retrievals,
            "successful_completions": self._successful_completions,
            "completion_rate": (
                    self._successful_completions / max(1, self._total_retrievals)
            ),
            "memory_utilization": len(self._memories) / self.config.memory_capacity,
        }

    def reset(self) -> None:
        """Reset CA3 network to initial state."""
        n = self.config.n_pyramidal_cells
        self._recurrent_weights = np.zeros((n, n), dtype=np.float32)
        self._memories = []
        self._pattern_to_memory = {}
        self._total_stores = 0
        self._total_retrievals = 0
        self._successful_completions = 0
