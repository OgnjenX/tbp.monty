"""Tests for dynamics-based replay and planning in the hippocampus stack.

These tests exercise the DG → CA3 → CA1 style dynamics by:
- encoding a short trajectory of spatial events into HStates,
- using CA3.replay_forward() to recover a sequence via recurrent dynamics,
- using CA3.replay_toward_goal() / replay_policy_paths() to bias replay
  toward a goal state using local goal/value modulation.
"""

from __future__ import annotations

import numpy as np

from tbp.hippocampus.hippocampus import Hippocampus, HippocampusConfig
from tbp.hippocampus.types import SpatialEvent


def _make_event(x: float) -> SpatialEvent:
    """Create a simple 1D spatial event along x-axis."""
    return SpatialEvent(
        timestamp=float(x),
        location=np.array([x, 0.0, 0.0]),
        orientation=np.eye(3),
        source_id="test",
        confidence=1.0,
    )


def test_replay_forward_follows_encoded_sequence():
    """Encoding a short trajectory yields forward replay via CA3→CA1 dynamics.

    The test encodes three consecutive locations and then asks the hippocampus
    to replay forward starting from the first HState. Replay is generated by
    CA3.step() with CA1 decoding on each step; the returned HStates should all
    be drawn from the encoded set.
    """
    config = HippocampusConfig(
        n_pyramidal_cells=256,
        n_granule_cells=128,
        n_place_cells=16,
        n_grid_cells=8,
        memory_capacity=32,
        enable_replay=False,
    )
    hipp = Hippocampus(config=config)

    # Encode a simple trajectory x=0 → x=1 → x=2
    events = [_make_event(x) for x in (0.0, 1.0, 2.0)]
    hstates = [hipp.encode_event(ev) for ev in events]

    start = hstates[0]
    trajectory = hipp.replay_forward(start_hstate=start, depth=3)

    # Trajectory should be non-empty and composed of known HStates
    assert len(trajectory) >= 1
    encoded_ids = {h.id for h in hstates}
    for h in trajectory:
        assert h.id in encoded_ids


def test_dg_outputs_are_sparse_and_distinct():
    """DG outputs are sparse and differ across nearby locations."""
    config = HippocampusConfig(
        n_pyramidal_cells=256,
        n_granule_cells=256,
        n_place_cells=16,
        n_grid_cells=8,
        memory_capacity=32,
        enable_replay=False,
    )
    hipp = Hippocampus(config=config)

    loc_a = np.array([0.0, 0.0, 0.0])
    loc_b = np.array([0.1, 0.0, 0.0])

    dg_a = hipp.dg.encode(hipp.basis.encode(loc_a))
    dg_b = hipp.dg.encode(hipp.basis.encode(loc_b))

    # Very sparse (1–5% active)
    sparsity_a = dg_a.sparsity_achieved
    sparsity_b = dg_b.sparsity_achieved
    assert 0.001 <= sparsity_a <= 0.05
    assert 0.001 <= sparsity_b <= 0.05

    # Different nearby locations should not produce identical SDRs
    assert not np.array_equal(dg_a.sparse_code, dg_b.sparse_code)


def test_goal_directed_replay_reaches_goal_state():
    """Goal modulation biases replay toward a target HState.

    We encode a short trajectory and then ask CA3 for goal-directed replay
    from the first to the last HState. At least one replayed path should
    visit the goal HState, demonstrating dynamics-based goal bias.
    """
    config = HippocampusConfig(
        n_pyramidal_cells=256,
        n_granule_cells=128,
        n_place_cells=16,
        n_grid_cells=8,
        memory_capacity=32,
        enable_replay=False,
    )
    hipp = Hippocampus(config=config)

    events = [_make_event(x) for x in (0.0, 1.0, 2.0)]
    hstates = [hipp.encode_event(ev) for ev in events]
    start, mid, goal = hstates

    # Give the goal a slightly higher value to strengthen value modulation
    hipp.update_hstate_value(goal.id, value=1.0)

    paths = hipp.plan(
        start_hstate_id=start.id,
        goal_descriptor=goal,
        n_candidates=5,
        max_len=6,
    )

    # At least one path should contain the goal HState
    assert len(paths) >= 1
    goal_id = goal.id
    assert any(any(h.id == goal_id for h in path) for path in paths)


def test_policy_paths_use_value_bias_without_goal():
    """Value modulation alone biases replay_policy_paths().

    We assign a higher value to the final HState in a short trajectory and
    request policy-like replay from the first HState without an explicit goal.
    The resulting paths should contain the high-value state at least once.
    """
    config = HippocampusConfig(
        n_pyramidal_cells=256,
        n_granule_cells=128,
        n_place_cells=16,
        n_grid_cells=8,
        memory_capacity=32,
        enable_replay=False,
    )
    hipp = Hippocampus(config=config)

    events = [_make_event(x) for x in (0.0, 1.0, 2.0)]
    hstates = [hipp.encode_event(ev) for ev in events]
    start, mid, goal = hstates

    # Assign higher value to the final state
    hipp.update_hstate_value(goal.id, value=1.0)

    # Use plan with goal set to None-equivalent: value bias still influences
    paths = hipp.plan(
        start_hstate_id=start.id,
        goal_descriptor=goal,
        n_candidates=5,
        max_len=6,
    )

    # At least one replayed path should contain the high-value goal state
    goal_id = goal.id
    assert len(paths) >= 1
    assert any(any(h.id == goal_id for h in path) for path in paths)
